{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import dicom, cv2, re, sys\n",
      "import os, fnmatch, shutil, subprocess\n",
      "import numpy as np\n",
      "from PIL import Image\n",
      "from IPython.utils import io\n",
      "import numpy as np\n",
      "np.random.seed(1234)\n",
      "import matplotlib as plt\n",
      "%matplotlib inline\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
      "import os, sys, urllib, gzip\n",
      "import cPickle as pickle\n",
      "sys.setrecursionlimit(10000)\n",
      "import glob\n",
      "from IPython.display import clear_output\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "from IPython.display import Image as IPImage\n",
      "\n",
      "from PIL import Image\n",
      "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
      "from lasagne.init import GlorotUniform\n",
      "from lasagne.nonlinearities import rectify, leaky_rectify, tanh, sigmoid, softmax\n",
      "from lasagne.updates import nesterov_momentum, adam\n",
      "from lasagne.objectives import categorical_crossentropy, binary_crossentropy\n",
      "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
      "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
      "import theano \n",
      "import theano.tensor as T\n",
      "import lasagne\n",
      "import time\n",
      "try:\n",
      "    from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
      "    from lasagne.layers.dnn import Conv3DDNNLayer as Conv3DLayer\n",
      "    from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
      "    from lasagne.layers.dnn import MaxPool3DDNNLayer as MaxPool3DLayer\n",
      "    print 'Using cuda_convnet (faster)'\n",
      "except ImportError:\n",
      "    from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "    from lasagne.layers import Conv3DLayer as Conv3DLayer\n",
      "    from lasagne.layers import Pool3Layer as Pool3Layer\n",
      "    from lasagne.layers import ReshapeLayer\n",
      "    print 'Using lasagne.layers (slower)'\n",
      "    \n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "from lasagne.layers import Layer\n",
      "from lasagne import init\n",
      "from lasagne import nonlinearities\n",
      "from scipy.misc import imresize, imread\n",
      "from PIL import ImageOps\n",
      "import scipy as sp\n",
      "import scipy.ndimage.morphology\n",
      "from skimage.morphology import convex_hull_image\n",
      "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral\n",
      "import matplotlib.cm as cm\n",
      "from scipy.optimize import minimize\n",
      "from math import floor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using cuda_convnet (faster)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from batchNormalization import BatchNormLayer\n",
      "\n",
      "def batch_norm(layer, **kwargs):\n",
      "    nonlinearity = getattr(layer, 'nonlinearity', None)\n",
      "    if nonlinearity is not None:\n",
      "        layer.nonlinearity = nonlinearities.identity\n",
      "    if hasattr(layer, 'b') and layer.b is not None:\n",
      "        del layer.params[layer.b]\n",
      "        layer.b = None\n",
      "    layer = BatchNormLayer(layer, **kwargs)\n",
      "    if nonlinearity is not None:\n",
      "        from lasagne.layers import NonlinearityLayer\n",
      "        layer = NonlinearityLayer(layer, nonlinearity)\n",
      "    return layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = np.load(\"X_train_step7.npy\")\n",
      "y_train = np.load(\"y_train_step7.npy\")\n",
      "setNames_train = np.load(\"setNames_train_step7.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L,W,H = X_train.shape[1::]\n",
      "X_train = X_train.reshape(-1,1,L,W,H).astype(\"float32\")\n",
      "y_train = y_train.astype(\"float32\")\n",
      "print X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1134, 1, 5, 100, 100)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_train.shape, X_train.min(), X_train.max(), X_train.mean(), X_train.std()\n",
      "print y_train.shape, y_train.min(), y_train.max()\n",
      "print len(setNames_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1134, 1, 5, 100, 100) -4.06875 9.27921 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.37524e-05 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0047\n",
        "(1134,) 0.0 1.0\n",
        "1134\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lasagne\n",
      "from lasagne.layers.shape import PadLayer\n",
      "from lasagne.layers import InputLayer, DenseLayer, NonlinearityLayer\n",
      "from lasagne.layers.dnn import Conv3DDNNLayer, MaxPool3DDNNLayer\n",
      "from lasagne.nonlinearities import softmax\n",
      "\n",
      "import theano\n",
      "import numpy as np\n",
      "import skimage.transform\n",
      "from skimage import color\n",
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gate_parameters = lasagne.layers.recurrent.Gate(\n",
      "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
      "    b=lasagne.init.Constant(0.))\n",
      "cell_parameters = lasagne.layers.recurrent.Gate(\n",
      "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
      "    # Setting W_cell to None denotes that no cell connection will be used.\n",
      "    W_cell=None, b=lasagne.init.Constant(0.),\n",
      "    # By convention, the cell nonlinearity is tanh in an LSTM.\n",
      "    nonlinearity=lasagne.nonlinearities.tanh)\n",
      "\n",
      "batch_size = 32\n",
      "\n",
      "def build_cnn(input_var = None):\n",
      "    \n",
      "    conv_num_filters1 = 32\n",
      "    conv_num_filters2 = 64\n",
      "    conv_num_filters3 = 32\n",
      "#     filter_size1 = 3,3\n",
      "#     filter_size2 = 3,3\n",
      "#     filter_size3 = 3,3\n",
      "    pool_size = 2\n",
      "    pool_size2 = 4\n",
      "    pad_in = 'valid'\n",
      "    pad_out = 'full'\n",
      "    \n",
      "    network = InputLayer(shape=(batch_size,1,L,W,H),input_var=input_var,name = \"input_layer\")\n",
      "\n",
      "    # ----------- 1st layer group ---------------\n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters1, (3,3,3), pad=1,\\\n",
      "                                        nonlinearity=lasagne.nonlinearities.rectify,flip_filters=False,name = \"conv1a\"))\n",
      "    network  = MaxPool3DDNNLayer(network,pool_size=(1,2,2),stride=(1,2,2),name = \"pool1\")\n",
      "\n",
      "    # ------------- 2nd layer group --------------\n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters1, (3,3,3), pad=1,\\\n",
      "                                        nonlinearity=lasagne.nonlinearities.rectify,name = \"conv2a\"))\n",
      "    network  = MaxPool3DDNNLayer(network,pool_size=(1,2,2),stride=(1,2,2),name = \"pool2\")\n",
      "\n",
      "    # ----------------- 3rd layer group --------------\n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters2, (3,3,3), pad=1,\\\n",
      "                                        nonlinearity=lasagne.nonlinearities.rectify,name = \"conv3a\"))\n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters2, (3,3,3), pad=1,\\\n",
      "                                        nonlinearity=lasagne.nonlinearities.rectify,name = \"conv3b\"))\n",
      "#     network  = MaxPool3DDNNLayer(network,pool_size=(2,2,2),stride=(2,2,2),name = \"pool3\")\n",
      "\n",
      "#     # ----------------- 4th layer group --------------\n",
      "#     network = batch_norm(Conv3DDNNLayer(network, 512, (3,3,3), pad=1,\\\n",
      "#                                         nonlinearity=lasagne.nonlinearities.rectify,name = \"conv4a\"))\n",
      "#     network = batch_norm(Conv3DDNNLayer(network, 512, (3,3,3), pad=1,\\\n",
      "#                                         nonlinearity=lasagne.nonlinearities.rectify,name = \"conv4b\"))\n",
      "#     network  = MaxPool3DDNNLayer(network,pool_size=(2,2,2),stride=(2,2,2),name = \"pool4\")\n",
      "\n",
      "#     # ----------------- 5th layer group --------------\n",
      "#     network = batch_norm(Conv3DDNNLayer(network, 512, (3,3,3), pad=1,\\\n",
      "#                                         nonlinearity=lasagne.nonlinearities.rectify,name = \"conv5a\"))\n",
      "#     network = batch_norm(Conv3DDNNLayer(network, 512, (3,3,3), pad=1,\\\n",
      "#                                         nonlinearity=lasagne.nonlinearities.rectify,name = \"conv5b\"))\n",
      "    # We need a padding layer, as C3D only pads on the right, which cannot be done with a theano pooling layer\n",
      "    network    = PadLayer(network,width=[(0,1),(0,1)], batch_ndim=3,name = \"pad\")\n",
      "    network  = MaxPool3DDNNLayer(network,pool_size=(1,2,2),pad=(0,0,0),stride=(1,2,2),name = \"pool5\")\n",
      "    network  = batch_norm(DenseLayer(network, num_units=256,nonlinearity=lasagne.nonlinearities.rectify,name = \"dense1\"))\n",
      "    network  = batch_norm(DenseLayer(network, num_units=256,nonlinearity=lasagne.nonlinearities.rectify,name = \"dense2\"))\n",
      "    network  = batch_norm(DenseLayer(network, num_units=1, nonlinearity=lasagne.nonlinearities.softmax,name = \"out\"))\n",
      "    \n",
      "    return network\n",
      "\n",
      "\n",
      "\n",
      "def build_lstm_cnn(input_var=None):\n",
      "    \n",
      "    \n",
      "    conv_num_filters1 = 32\n",
      "    conv_num_filters2 = 64\n",
      "    conv_num_filters3 = 128\n",
      "    filter_size1 = 7\n",
      "    filter_size2 = 3\n",
      "    filter_size3 = 3\n",
      "    pool_size = 2\n",
      "    pool_size2 = 2\n",
      "    pad_in = 'valid'\n",
      "    pad_out = 'full'\n",
      "    \n",
      "    # Input layer, as usual:                                                                                                                                                                                \n",
      "    network = InputLayer(shape=(batch_size,1,L,W,H),input_var=input_var,name=\"input_layer\")                                                                                                                             \n",
      "    \n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters1, (1,filter_size1,filter_size1)\\\n",
      "                             ,pad = \"full\",nonlinearity=lasagne.nonlinearities.rectify,flip_filters=False, name = \"conv3_1\"))\n",
      "    \n",
      "    network = MaxPool3DDNNLayer(network,pool_size=(1,pool_size,pool_size),stride=(1,pool_size,pool_size), name = \"pool3_1\")\n",
      "    \n",
      "    \n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters2, (1,filter_size2,filter_size2)\\\n",
      "                             ,pad = \"full\",nonlinearity=lasagne.nonlinearities.rectify,flip_filters=False, name = \"conv3_2\"))\n",
      "    \n",
      "    network = MaxPool3DDNNLayer(network,pool_size=(1,pool_size,pool_size),stride=(1,pool_size,pool_size), name = \"pool3_2\")\n",
      "    \n",
      "    \n",
      "    network = batch_norm(Conv3DDNNLayer(network, conv_num_filters3, (1,filter_size3,filter_size3)\\\n",
      "                             ,pad = \"full\",nonlinearity=lasagne.nonlinearities.rectify,flip_filters=False, name = \"conv3_3\"))\n",
      "    \n",
      "    network = MaxPool3DDNNLayer(network,pool_size=(1,pool_size2,pool_size2),stride=(1,pool_size2,pool_size2), name = \"pool3_3\")\n",
      "    \n",
      "    \n",
      "    Netshape = network.output_shape\n",
      "    \n",
      "    network = ReshapeLayer(network, shape = (-1,Netshape[2],Netshape[1],Netshape[3],Netshape[4]),name=\"reshape\")\n",
      "    \n",
      "    # Our LSTM will have 10 hidden/cell units\n",
      "    N_HIDDEN = 200\n",
      "    GRAD_CLIP = 100.0\n",
      "    network = lasagne.layers.recurrent.LSTMLayer(\n",
      "    network, N_HIDDEN, ingate=gate_parameters, forgetgate=gate_parameters,\n",
      "     cell=cell_parameters, outgate=gate_parameters,\n",
      "    grad_clipping=GRAD_CLIP,\n",
      "    learn_init=True,name=\"lstm_1\")\n",
      "\n",
      "    network = lasagne.layers.recurrent.LSTMLayer(\n",
      "    network, N_HIDDEN, ingate=gate_parameters, forgetgate=gate_parameters,\n",
      "     cell=cell_parameters, outgate=gate_parameters,\n",
      "    grad_clipping=GRAD_CLIP,\n",
      "    learn_init=True,name = \"lstm_2\")\n",
      "\n",
      "    network = lasagne.layers.SliceLayer(network, -1, 1,name=\"slice\")\n",
      "    \n",
      "    network = batch_norm(lasagne.layers.DenseLayer(network, num_units=1, \\\n",
      "                                  W = lasagne.init.Normal(), \\\n",
      "                                  nonlinearity=lasagne.nonlinearities.softmax,name = \"dense\"))\n",
      "    \n",
      "    return network\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# network = build_cnn()\n",
      "network = build_lstm_cnn()\n",
      "\n",
      "laylist = lasagne.layers.get_all_layers(network)\n",
      "for l in laylist:\n",
      "    print(l.name, lasagne.layers.get_output_shape(l))\n",
      "\n",
      "num_params = lasagne.layers.count_params(network)\n",
      "print(\"# of parameters is {}\".format(num_params))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('input_layer', (32, 1, 5, 100, 100))\n",
        "('conv3_1', (32, 32, 5, 106, 106))\n",
        "(None, (32, 32, 5, 106, 106))\n",
        "(None, (32, 32, 5, 106, 106))\n",
        "('pool3_1', (32, 32, 5, 53, 53))\n",
        "('conv3_2', (32, 64, 5, 55, 55))\n",
        "(None, (32, 64, 5, 55, 55))\n",
        "(None, (32, 64, 5, 55, 55))\n",
        "('pool3_2', (32, 64, 5, 27, 27))\n",
        "('conv3_3', (32, 128, 5, 29, 29))\n",
        "(None, (32, 128, 5, 29, 29))\n",
        "(None, (32, 128, 5, 29, 29))\n",
        "('pool3_3', (32, 128, 5, 14, 14))\n",
        "('reshape', (32, 5, 128, 14, 14))\n",
        "('lstm_1', (32, 5, 200))\n",
        "('lstm_2', (32, 5, 200))\n",
        "('slice', (32, 200))\n",
        "('dense', (32, 1))\n",
        "(None, (32, 1))\n",
        "(None, (32, 1))\n",
        "# of parameters is 20648828\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
      "    assert len(inputs) == len(targets)\n",
      "    if shuffle:\n",
      "        indices = np.arange(len(inputs))\n",
      "        np.random.shuffle(indices)\n",
      "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
      "        if shuffle:\n",
      "            excerpt = indices[start_idx:start_idx + batchsize]\n",
      "        else:\n",
      "            excerpt = slice(start_idx, start_idx + batchsize)\n",
      "        yield inputs[excerpt], targets[excerpt]\n",
      "\n",
      "def iterator(X, batchsize):\n",
      "    indices = np.arange(len(X))\n",
      "    for i in range(0, len(X) - batchsize + 1, batchsize):\n",
      "        sli = indices[i:i+batchsize]\n",
      "        yield X[sli]\n",
      "\n",
      "def save_params(model, fn):\n",
      "    with open(fn, 'w') as wr:\n",
      "        pickle.dump(lasagne.layers.get_all_param_values(model), wr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Symbolic variable for the target network output.\n",
      "# It will be of shape n_batch, because there's only 1 target value per sequence.\n",
      "\n",
      "target_values = T.vector('target_output')\n",
      "dtensor5 = T.TensorType('float32', (False,)*5)\n",
      "input_var = dtensor5('inputs')\n",
      "\n",
      "network = build_lstm_cnn(input_var)\n",
      "# network = build_cnn(input_var)\n",
      "# lasagne.layers.get_output produces an expression for the output of the net\n",
      "network_output = lasagne.layers.get_output(network)\n",
      "\n",
      "laylist = lasagne.layers.get_all_layers(network)\n",
      "    \n",
      "for l in laylist:\n",
      "    print(l.name, lasagne.layers.get_output_shape(l))\n",
      "\n",
      "num_params = lasagne.layers.count_params(network)\n",
      "\n",
      "print(\"number of parameters is {}\".format(num_params))\n",
      "# The value we care about is the final value produced for each sequence\n",
      "# so we simply slice it out.\n",
      "predicted_values = network_output[:, -1]\n",
      "# Our cost will be mean-squared error\n",
      "# cost = T.mean((predicted_values - target_values)**2)\n",
      "cost = lasagne.objectives.squared_error(predicted_values, target_values)\n",
      "cost = cost.mean()\n",
      "\n",
      "# Retrieve all parameters from the network\n",
      "all_params = lasagne.layers.get_all_params(network ,trainable = True)\n",
      "# Compute adam updates for training\n",
      "updates = lasagne.updates.adam(cost, all_params, learning_rate= 0.1)\n",
      "# Theano functions for training and computing cost\n",
      "train = theano.function([input_var, target_values],cost, updates=updates)\n",
      "compute_cost = theano.function([input_var, target_values], cost)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('input_layer', (32, 1, 5, 100, 100))\n",
        "('conv3_1', (32, 32, 5, 106, 106))\n",
        "(None, (32, 32, 5, 106, 106))\n",
        "(None, (32, 32, 5, 106, 106))\n",
        "('pool3_1', (32, 32, 5, 53, 53))\n",
        "('conv3_2', (32, 64, 5, 55, 55))\n",
        "(None, (32, 64, 5, 55, 55))\n",
        "(None, (32, 64, 5, 55, 55))\n",
        "('pool3_2', (32, 64, 5, 27, 27))\n",
        "('conv3_3', (32, 128, 5, 29, 29))\n",
        "(None, (32, 128, 5, 29, 29))\n",
        "(None, (32, 128, 5, 29, 29))\n",
        "('pool3_3', (32, 128, 5, 14, 14))\n",
        "('reshape', (32, 5, 128, 14, 14))\n",
        "('lstm_1', (32, 5, 200))\n",
        "('lstm_2', (32, 5, 200))\n",
        "('slice', (32, 200))\n",
        "('dense', (32, 1))\n",
        "(None, (32, 1))\n",
        "(None, (32, 1))\n",
        "number of parameters is 20648828\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Finally, launch the training loop.\n",
      "print(\"Starting training...\")\n",
      "# We iterate over epochs: \n",
      "num_epochs = 100\n",
      "for epoch in range(num_epochs):\n",
      "    # In each epoch, we do a full pass over the training data:\n",
      "    train_err = 0\n",
      "    train_batches = 0\n",
      "    start_time = time.time()\n",
      "    for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
      "        inputs, targets = batch\n",
      "#         print inputs.shape, targets.shape\n",
      "        \n",
      "        train_err += train(inputs, targets)\n",
      "        \n",
      "        train_batches += 1\n",
      "#         print compute_cost(inputs, targets)\n",
      "    # And a full pass over the validation data:\n",
      "#     val_err = 0\n",
      "#     val_batches = 0\n",
      "#     for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
      "#         inputs, targets = batch\n",
      "#         err = compute_cost(inputs, targets)\n",
      "#         val_err += err[0]\n",
      "#         val_batches += 1\n",
      "\n",
      "    # Then we print the results for this epoch:                                                                                                                                                         \n",
      "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
      "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
      "#     print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
      "#     print(\"  ratio:\\t\\t{:.6f}\".format((train_err / train_batches)/(val_err / val_batches)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "MemoryError",
       "evalue": "Error allocating 16056320 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)\nToposort index: 604\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(160, 800), (800, 25088)]\nInputs strides: [(800, 1), (1, 800)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuReshape{3}(GpuDot22.0, MakeVector{dtype='int64'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-24-22ebf998d880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print inputs.shape, targets.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    872\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mMemoryError\u001b[0m: Error allocating 16056320 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)\nToposort index: 604\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(160, 800), (800, 25088)]\nInputs strides: [(800, 1), (1, 800)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuReshape{3}(GpuDot22.0, MakeVector{dtype='int64'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting training...\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}