{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lasagne.layers (slower)\n"
     ]
    }
   ],
   "source": [
    "# import dicom, cv2, re, sys\n",
    "import os, fnmatch, shutil, subprocess\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
    "import os, sys, urllib, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "from PIL import Image\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.init import GlorotUniform\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh, sigmoid, softmax\n",
    "from lasagne.updates import nesterov_momentum, adam\n",
    "from lasagne.objectives import categorical_crossentropy, binary_crossentropy\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
    "import theano \n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import time\n",
    "try:\n",
    "    from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
    "    from lasagne.layers.dnn import Conv3DDNNLayer as Conv3DLayer\n",
    "    from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
    "    print 'Using cuda_convnet (faster)'\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
    "    from lasagne.layers import Conv3DLayer as Conv3DLayer\n",
    "    from lasagne.layers import Pool3Layer as Pool3Layer\n",
    "    from lasagne.layers import ReshapeLayer\n",
    "    print 'Using lasagne.layers (slower)'\n",
    "    \n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne.layers import Layer\n",
    "from lasagne import init\n",
    "from lasagne import nonlinearities\n",
    "from scipy.misc import imresize, imread\n",
    "from PIL import ImageOps\n",
    "import scipy as sp\n",
    "import scipy.ndimage.morphology\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral\n",
    "import matplotlib.cm as cm\n",
    "from scipy.optimize import minimize\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from batchNormalization import BatchNormLayer\n",
    "\n",
    "def batch_norm(layer, **kwargs):\n",
    "    nonlinearity = getattr(layer, 'nonlinearity', None)\n",
    "    if nonlinearity is not None:\n",
    "        layer.nonlinearity = nonlinearities.identity\n",
    "    if hasattr(layer, 'b') and layer.b is not None:\n",
    "        del layer.params[layer.b]\n",
    "        layer.b = None\n",
    "    layer = BatchNormLayer(layer, **kwargs)\n",
    "    if nonlinearity is not None:\n",
    "        from lasagne.layers import NonlinearityLayer\n",
    "        layer = NonlinearityLayer(layer, nonlinearity)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train_step12.npy\")\n",
    "y_train = np.load(\"y_train_step12.npy\")\n",
    "setNames_train = np.load(\"setNames_train_step12.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630, 1, 5, 225, 225)\n"
     ]
    }
   ],
   "source": [
    "L,W,H = X_train.shape[1::]\n",
    "X_train = X_train.reshape(-1,1,L,W,H).astype(\"float32\")\n",
    "y_train = y_train.astype(\"float32\")\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.))\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    # Setting W_cell to None denotes that no cell connection will be used.\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),\n",
    "    # By convention, the cell nonlinearity is tanh in an LSTM.\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def build_lstm_cnn(input_var=None):\n",
    "    \n",
    "    \n",
    "    conv_num_filters1 = 4\n",
    "    conv_num_filters2 = 8\n",
    "    conv_num_filters3 = 8\n",
    "    filter_size1 = 7\n",
    "    filter_size2 = 3\n",
    "    filter_size3 = 3\n",
    "    pool_size = 2\n",
    "    pool_size2 = 4\n",
    "    pad_in = 'valid'\n",
    "    pad_out = 'full'\n",
    "    \n",
    "    # Input layer, as usual:                                                                                                                                                                                \n",
    "    network = InputLayer(shape=(batch_size,1,L,W,H),input_var=input_var,name=\"input_layer\")                                                                                                                             \n",
    "        \n",
    "    network = batch_norm(Conv3DLayer(\n",
    "            network, num_filters=conv_num_filters1, filter_size=(1,filter_size1, filter_size1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv3_1\"))\n",
    "    \n",
    "    network = Pool3Layer(network, pool_size=(1,pool_size, pool_size),name=\"pool3_1\")\n",
    "    \n",
    "    network = batch_norm(Conv3DLayer(\n",
    "            network, num_filters=conv_num_filters2, filter_size=(1,filter_size2, filter_size2),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv3_2\"))\n",
    "    \n",
    "    network = Pool3Layer(network, pool_size=(1,pool_size, pool_size),name=\"pool3_2\")\n",
    "    \n",
    "    network = batch_norm(Conv3DLayer(\n",
    "            network, num_filters=conv_num_filters3, filter_size=(1,filter_size3, filter_size3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv3_3\"))\n",
    "    \n",
    "    network = Pool3Layer(network, pool_size=(1,pool_size2, pool_size2),name=\"pool3_3\")\n",
    "    \n",
    "    Netshape = network.output_shape\n",
    "\n",
    "    network = ReshapeLayer(network, shape = (-1,Netshape[2],Netshape[1],Netshape[3],Netshape[4]),name=\"reshape\")\n",
    "    \n",
    "    # Our LSTM will have 10 hidden/cell units\n",
    "    N_HIDDEN = 20\n",
    "    GRAD_CLIP = 100.0\n",
    "    network = lasagne.layers.recurrent.LSTMLayer(\n",
    "    network, N_HIDDEN, ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "     cell=cell_parameters, outgate=gate_parameters,\n",
    "    grad_clipping=GRAD_CLIP,\n",
    "    learn_init=True,name=\"lstm_1\")\n",
    "\n",
    "    network = lasagne.layers.recurrent.LSTMLayer(\n",
    "    network, N_HIDDEN, ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "     cell=cell_parameters, outgate=gate_parameters,\n",
    "    grad_clipping=GRAD_CLIP,\n",
    "    learn_init=True,name = \"lstm_2\")\n",
    "\n",
    "    network = lasagne.layers.SliceLayer(network, -1, 1,name=\"slice\")\n",
    "    \n",
    "    network = batch_norm(lasagne.layers.DenseLayer(network, num_units=1, \\\n",
    "                                  W = lasagne.init.Normal(), \\\n",
    "                                  nonlinearity=lasagne.nonlinearities.softmax,name = \"dense\"))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build a net and make sure we have the dimensions right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer', (2, 1, 5, 225, 225))\n",
      "('conv3_1', (2, 4, 5, 219, 219))\n",
      "(None, (2, 4, 5, 219, 219))\n",
      "(None, (2, 4, 5, 219, 219))\n",
      "('pool3_1', (2, 4, 5, 109, 109))\n",
      "('conv3_2', (2, 8, 5, 107, 107))\n",
      "(None, (2, 8, 5, 107, 107))\n",
      "(None, (2, 8, 5, 107, 107))\n",
      "('pool3_2', (2, 8, 5, 53, 53))\n",
      "('conv3_3', (2, 8, 5, 51, 51))\n",
      "(None, (2, 8, 5, 51, 51))\n",
      "(None, (2, 8, 5, 51, 51))\n",
      "('pool3_3', (2, 8, 5, 12, 12))\n",
      "('reshape', (2, 5, 8, 12, 12))\n",
      "('lstm_1', (2, 5, 20))\n",
      "('lstm_2', (2, 5, 20))\n",
      "('slice', (2, 20))\n",
      "('dense', (2, 1))\n",
      "(None, (2, 1))\n",
      "(None, (2, 1))\n",
      "# of parameters is 98484\n"
     ]
    }
   ],
   "source": [
    "network = build_lstm_cnn()\n",
    "\n",
    "laylist = lasagne.layers.get_all_layers(network)\n",
    "for l in laylist:\n",
    "    print(l.name, lasagne.layers.get_output_shape(l))\n",
    "\n",
    "num_params = lasagne.layers.count_params(network)\n",
    "print(\"# of parameters is {}\".format(num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "def iterator(X, batchsize):\n",
    "    indices = np.arange(len(X))\n",
    "    for i in range(0, len(X) - batchsize + 1, batchsize):\n",
    "        sli = indices[i:i+batchsize]\n",
    "        yield X[sli]\n",
    "\n",
    "def save_params(model, fn):\n",
    "    with open(fn, 'w') as wr:\n",
    "        pickle.dump(lasagne.layers.get_all_param_values(model), wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer', (2, 1, 5, 225, 225))\n",
      "('conv3_1', (2, 4, 5, 219, 219))\n",
      "(None, (2, 4, 5, 219, 219))\n",
      "(None, (2, 4, 5, 219, 219))\n",
      "('pool3_1', (2, 4, 5, 109, 109))\n",
      "('conv3_2', (2, 8, 5, 107, 107))\n",
      "(None, (2, 8, 5, 107, 107))\n",
      "(None, (2, 8, 5, 107, 107))\n",
      "('pool3_2', (2, 8, 5, 53, 53))\n",
      "('conv3_3', (2, 8, 5, 51, 51))\n",
      "(None, (2, 8, 5, 51, 51))\n",
      "(None, (2, 8, 5, 51, 51))\n",
      "('pool3_3', (2, 8, 5, 12, 12))\n",
      "('reshape', (2, 5, 8, 12, 12))\n",
      "('lstm_1', (2, 5, 20))\n",
      "('lstm_2', (2, 5, 20))\n",
      "('slice', (2, 20))\n",
      "('dense', (2, 1))\n",
      "(None, (2, 1))\n",
      "(None, (2, 1))\n",
      "number of parameters is 98484\n"
     ]
    }
   ],
   "source": [
    "# Symbolic variable for the target network output.\n",
    "# It will be of shape n_batch, because there's only 1 target value per sequence.\n",
    "\n",
    "target_values = T.vector('target_output')\n",
    "dtensor5 = T.TensorType('float32', (False,)*5)\n",
    "input_var = dtensor5('inputs')\n",
    "\n",
    "network = build_lstm_cnn(input_var)\n",
    "# lasagne.layers.get_output produces an expression for the output of the net\n",
    "network_output = lasagne.layers.get_output(network)\n",
    "\n",
    "laylist = lasagne.layers.get_all_layers(network)\n",
    "    \n",
    "for l in laylist:\n",
    "    print(l.name, lasagne.layers.get_output_shape(l))\n",
    "\n",
    "num_params = lasagne.layers.count_params(network)\n",
    "\n",
    "print(\"number of parameters is {}\".format(num_params))\n",
    "# The value we care about is the final value produced for each sequence\n",
    "# so we simply slice it out.\n",
    "predicted_values = network_output[:, -1]\n",
    "# Our cost will be mean-squared error\n",
    "# cost = T.mean((predicted_values - target_values)**2)\n",
    "cost = lasagne.objectives.squared_error(predicted_values, target_values)\n",
    "cost = cost.mean()\n",
    "\n",
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(network ,trainable = True)\n",
    "# Compute adam updates for training\n",
    "updates = lasagne.updates.adam(cost, all_params)\n",
    "# Theano functions for training and computing cost\n",
    "train = theano.function([input_var, target_values],cost, updates=updates)\n",
    "compute_cost = theano.function([input_var, target_values], cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "(2, 1, 5, 225, 225) (2,)\n",
      "i0 is 1, i1 is 3\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "is this allowed?\nApply node that caused the error: DiagonalSubtensor{inplace}(Reshape{6}.0, TensorConstant{1}, TensorConstant{3})\nToposort index: 153\nInputs types: [TensorType(float32, 6D), TensorType(int8, scalar), TensorType(int8, scalar)]\nInputs shapes: [(2, 5, 4, 7, 225, 219), (), ()]\nInputs strides: [(27594000, 5518800, 1379700, 197100, 876, 4), (), ()]\nInputs values: ['not shown', array(1, dtype=int8), array(3, dtype=int8)]\nOutputs clients: [[Sum{axis=[3], acc_dtype=float64}(DiagonalSubtensor{inplace}.0), Shape_i{1}(DiagonalSubtensor{inplace}.0), Shape_i{4}(DiagonalSubtensor{inplace}.0), Shape_i{5}(DiagonalSubtensor{inplace}.0), Shape_i{2}(DiagonalSubtensor{inplace}.0), Shape_i{0}(DiagonalSubtensor{inplace}.0), Shape_i{3}(DiagonalSubtensor{inplace}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-bb42d0f95a57>\", line 10, in <module>\n    network_output = lasagne.layers.get_output(network)\n  File \"/Users/louis/Lasagne/lasagne/layers/helper.py\", line 191, in get_output\n    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)\n  File \"/Users/louis/Lasagne/lasagne/layers/conv.py\", line 332, in get_output_for\n    conved = self.convolve(input, **kwargs)\n  File \"/Users/louis/Lasagne/lasagne/layers/conv.py\", line 723, in convolve\n    self.shuffled_shape, self.suffled_W_shape,)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8e699964d3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         train_batches += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louis/Documents/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    913\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louis/Documents/Theano/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louis/Documents/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louis/Documents/Theano/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louis/Documents/Theano/theano/tensor/nnet/conv3d2d.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inputs, output_storage)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mperform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mxview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_diagonal_subtensor_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louis/Documents/Theano/theano/tensor/nnet/conv3d2d.py\u001b[0m in \u001b[0;36mget_diagonal_subtensor_view\u001b[0;34m(x, i0, i1)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i0 is {}, i1 is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is this allowed?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: is this allowed?\nApply node that caused the error: DiagonalSubtensor{inplace}(Reshape{6}.0, TensorConstant{1}, TensorConstant{3})\nToposort index: 153\nInputs types: [TensorType(float32, 6D), TensorType(int8, scalar), TensorType(int8, scalar)]\nInputs shapes: [(2, 5, 4, 7, 225, 219), (), ()]\nInputs strides: [(27594000, 5518800, 1379700, 197100, 876, 4), (), ()]\nInputs values: ['not shown', array(1, dtype=int8), array(3, dtype=int8)]\nOutputs clients: [[Sum{axis=[3], acc_dtype=float64}(DiagonalSubtensor{inplace}.0), Shape_i{1}(DiagonalSubtensor{inplace}.0), Shape_i{4}(DiagonalSubtensor{inplace}.0), Shape_i{5}(DiagonalSubtensor{inplace}.0), Shape_i{2}(DiagonalSubtensor{inplace}.0), Shape_i{0}(DiagonalSubtensor{inplace}.0), Shape_i{3}(DiagonalSubtensor{inplace}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-bb42d0f95a57>\", line 10, in <module>\n    network_output = lasagne.layers.get_output(network)\n  File \"/Users/louis/Lasagne/lasagne/layers/helper.py\", line 191, in get_output\n    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)\n  File \"/Users/louis/Lasagne/lasagne/layers/conv.py\", line 332, in get_output_for\n    conved = self.convolve(input, **kwargs)\n  File \"/Users/louis/Lasagne/lasagne/layers/conv.py\", line 723, in convolve\n    self.shuffled_shape, self.suffled_W_shape,)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs: \n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        print inputs.shape, targets.shape\n",
    "        \n",
    "        train_err += train(inputs, targets)\n",
    "        break\n",
    "#         train_batches += 1\n",
    "#             print train_fn2(inputs, targets)\n",
    "    # And a full pass over the validation data:\n",
    "#     val_err = 0\n",
    "#     val_batches = 0\n",
    "#     for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
    "#         inputs, targets = batch\n",
    "#         err = compute_cost(inputs, targets)\n",
    "#         val_err += err[0]\n",
    "#         val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:                                                                                                                                                         \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#     print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "#     print(\"  ratio:\\t\\t{:.6f}\".format((train_err / train_batches)/(val_err / val_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
